# Investigation: Time-Balanced Test Chunk Optimization

## Objective
Investigate using historical execution times to balance test chunks by total execution time 
rather than just file count, ensuring each chunk takes approximately the same time to complete.

## Current State Analysis

### Current Distribution (by file count)
- Chunk 0: 4 files
  - spec/arguments_spec.sh
  - spec/bin/git.conventional-commits_spec.sh
  - spec/bin/git.verify-all-commits_spec.sh
  - spec/bin/npm.versions_spec.sh
- Chunk 1: 4 files
  - spec/commons_spec.sh
  - spec/dependencies_spec.sh
  - spec/dryrun_spec.sh
  - spec/git_semantic_version_spec.sh
- Chunk 2: 4 files
  - spec/installation_spec.sh
  - spec/logger_spec.sh
  - spec/traps_nested_spec.sh
  - spec/traps_spec.sh
- Chunk 3: 1 files
  - spec/version-up_spec.sh

## Available Timing Data Sources

### 1. JUnit XML Reports (shellspec --output junit)

ShellSpec generates JUnit XML in the `report/` directory. JUnit XML format includes:
- `<testcase time="X.XXX">` - Individual test execution time
- Per-file aggregation possible by parsing XML

**Pros:**
- Already generated by current CI
- Standard XML format, easy to parse
- Per-test granularity available

**Cons:**
- Requires XML parsing
- Only available after first run
- Need to aggregate by file


### 2. GitHub Actions Job Timing

GitHub Actions provides timing data via:
- Workflow run API: `GET /repos/{owner}/{repo}/actions/runs/{run_id}`
- Job timing: `GET /repos/{owner}/{repo}/actions/runs/{run_id}/jobs`
- Step-level timing in job data

**Example API response structure:**
```json
{
  "jobs": [
    {
      "name": "Ubuntu (chunk 0)",
      "started_at": "2024-01-01T10:00:00Z",
      "completed_at": "2024-01-01T10:05:30Z",
      "steps": [
        {
          "name": "Run shellspec tests with coverage (chunk 0)",
          "started_at": "2024-01-01T10:02:00Z",
          "completed_at": "2024-01-01T10:05:00Z"
        }
      ]
    }
  ]
}
```

**Pros:**
- High-level chunk timing readily available
- No parsing needed
- Accessible via GitHub API or gh CLI

**Cons:**
- Chunk-level only, not per-file
- Includes setup/teardown overhead
- Requires API calls or gh CLI


### 3. Test Artifacts from Previous Runs

GitHub Actions artifacts are available for 30 days (current retention):
- Download timing data from previous successful runs
- Store as JSON/CSV in repository or cache
- Use `actions/cache` or commit to a timing-data branch

**Pros:**
- Persistent historical data
- Can track trends over time
- Simple CSV/JSON format

**Cons:**
- Requires workflow changes
- 30-day retention limit for artifacts
- Need bootstrap data for first run

### 4. Static Complexity Metrics (Fallback)

Use test file size and test count as proxies for execution time:
- Line count: `wc -l spec/*_spec.sh`
- Test case count: `grep -c "It\|Describe" spec/*_spec.sh`
- Cyclomatic complexity via shellcheck metrics

**Pros:**
- No historical data needed
- Works immediately
- Reasonably accurate correlation

**Cons:**
- Less accurate than actual timing
- Doesn't account for I/O, sleep, external calls
- May miss slow individual tests


## Current Test Complexity Analysis

Let's analyze test complexity as a proxy for execution time:

### Test File Metrics

| File | Lines | Test Blocks | Estimated Weight |
|------|-------|-------------|------------------|
| arguments_spec.sh | 292 | 15 | 442 |
| bin/git.conventional-commits_spec.sh | 346 | 36 | 706 |
| bin/git.verify-all-commits_spec.sh | 344 | 33 | 674 |
| bin/npm.versions_spec.sh | 558 | 56 | 1118 |
| commons_spec.sh | 856 | 63 | 1486 |
| dependencies_spec.sh | 410 | 38 | 790 |
| dryrun_spec.sh | 507 | 56 | 1067 |
| git_semantic_version_spec.sh | 697 | 118 | 1877 |
| installation_spec.sh | 1448 | 98 | 2428 |
| logger_spec.sh | 283 | 27 | 553 |
| traps_nested_spec.sh | 340 | 25 | 590 |
| traps_spec.sh | 430 | 46 | 890 |
| version-up_spec.sh | 1253 | 57 | 1823 |

### Current Chunk Weight Distribution

**Current distribution (alphabetically sorted, then chunked):**

- **Chunk 0**: Weight = 2940
  - arguments_spec.sh (442)
  - bin/git.conventional-commits_spec.sh (706)
  - bin/git.verify-all-commits_spec.sh (674)
  - bin/npm.versions_spec.sh (1118)

- **Chunk 1**: Weight = 5220
  - commons_spec.sh (1486)
  - dependencies_spec.sh (790)
  - dryrun_spec.sh (1067)
  - git_semantic_version_spec.sh (1877)

- **Chunk 2**: Weight = 4461
  - installation_spec.sh (2428) ⚠️ HEAVY
  - logger_spec.sh (553)
  - traps_nested_spec.sh (590)
  - traps_spec.sh (890)

- **Chunk 3**: Weight = 1823
  - version-up_spec.sh (1823)

**Statistics:**
- Total weight: 14444
- Average per chunk: 3611
- Chunk 0: 2940 (-19% vs avg)
- Chunk 1: 5220 (+44% vs avg) ⚠️ HEAVIEST
- Chunk 2: 4461 (+23% vs avg) ⚠️ HEAVY
- Chunk 3: 1823 (-50% vs avg) ⚠️ LIGHTEST

**Imbalance detected:** 
- Chunk 1 is ~77% heavier than Chunk 0
- Chunk 1 is ~186% heavier than Chunk 3
- This means Chunk 1 could take 2-3x longer to complete!

## Optimized Chunk Distribution (Bin-Packing Algorithm)

**Algorithm:** First Fit Decreasing (FFD)
- Sort tests by weight descending
- Assign each test to the currently lightest chunk

- **Chunk 0**: Weight = 3771 (+4% vs avg)
  - installation_spec.sh (2428)
  - dependencies_spec.sh (790)
  - logger_spec.sh (553)

- **Chunk 1**: Weight = 3441 (-5% vs avg)
  - git_semantic_version_spec.sh (1877)
  - traps_spec.sh (890)
  - bin/git.verify-all-commits_spec.sh (674)

- **Chunk 2**: Weight = 3480 (-4% vs avg)
  - version-up_spec.sh (1823)
  - dryrun_spec.sh (1067)
  - traps_nested_spec.sh (590)

- **Chunk 3**: Weight = 3752 (+4% vs avg)
  - commons_spec.sh (1486)
  - bin/npm.versions_spec.sh (1118)
  - bin/git.conventional-commits_spec.sh (706)
  - arguments_spec.sh (442)

**Statistics:**
- Total weight: 14444
- Average per chunk: 3611
- Min chunk: 3441 (-5% vs avg)
- Max chunk: 3771 (+4% vs avg)
- Range: 330 (9% of average)

**Improvement:**
- Old range: 3397 (3397)
- New range: 330
- Improvement: 90% reduction in imbalance

## Implementation Approaches

### Approach 1: Static Weight-Based Chunking (Recommended for MVP)

**Implementation:**
1. Calculate static weights at repository analysis time
2. Hard-code optimal distribution in `chunk-tests.sh`
3. Recalculate when test suite changes significantly

**Pros:**
- ✅ No runtime overhead
- ✅ Immediate 90% imbalance improvement
- ✅ No external dependencies
- ✅ Simple implementation (~50 lines of bash)
- ✅ Deterministic and predictable

**Cons:**
- ❌ Needs manual recalculation when tests change
- ❌ Doesn't adapt to actual runtime differences
- ❌ Weight formula may not perfectly match execution time

**Effort:** 1-2 hours

**Code snippet:**
```bash
# In chunk-tests.sh, replace alphabetical sort with:
case $CHUNK_INDEX in
  0) echo "spec/installation_spec.sh spec/dependencies_spec.sh spec/logger_spec.sh" ;;
  1) echo "spec/git_semantic_version_spec.sh spec/traps_spec.sh spec/bin/git.verify-all-commits_spec.sh" ;;
  # ... etc
esac
```

---

### Approach 2: JUnit XML Timing Data

**Implementation:**
1. Download JUnit XML artifacts from previous CI run
2. Parse `<testcase time="X.XXX">` attributes
3. Aggregate by spec file
4. Run bin-packing algorithm with actual timings
5. Store result in cache or commit to repo

**Pros:**
- ✅ Uses actual execution times
- ✅ Accounts for I/O, sleeps, external calls
- ✅ Can be automated in CI
- ✅ Self-improving over time

**Cons:**
- ❌ Requires XML parsing (xmllint, python, etc.)
- ❌ Needs successful previous run
- ❌ ~2-3 minute overhead to download/process artifacts
- ❌ More complex implementation

**Effort:** 4-6 hours

**Workflow integration:**
```yaml
- name: Download previous test timings
  uses: dawidd6/action-download-artifact@v2
  with:
    workflow: shellspec.yaml
    name: test-results-ubuntu-chunk-0
    path: /tmp/previous-timings/

- name: Calculate optimal chunks from timing data
  run: |
    python3 bin/analyze-test-timings.py /tmp/previous-timings/*.xml > .test-timings.json
    git add .test-timings.json
```

---

### Approach 3: GitHub Actions API Timing

**Implementation:**
1. Query GitHub API for previous workflow run timings
2. Extract step-level timing for each chunk
3. Use chunk timing as proxy for file weights
4. Redistribute files across chunks

**Pros:**
- ✅ No artifact downloads needed
- ✅ Simple JSON parsing
- ✅ Available via `gh` CLI

**Cons:**
- ❌ Only chunk-level timing (less granular)
- ❌ Includes setup/teardown overhead
- ❌ Requires GitHub token
- ❌ Iterative approach (takes multiple runs to converge)

**Effort:** 3-4 hours

**Example:**
```bash
gh api repos/OleksandrKucherenko/e-bash/actions/runs --jq '.workflow_runs[0].id'
gh api repos/OleksandrKucherenko/e-bash/actions/runs/12345/jobs \
  --jq '.jobs[] | select(.name | contains("chunk")) | {name, duration: (.completed_at - .started_at)}'
```

---

### Approach 4: Hybrid (Static + Periodic Update)

**Implementation:**
1. Start with static weight-based distribution (Approach 1)
2. Weekly/monthly scheduled workflow updates weights from JUnit XML
3. Automated PR to update chunk distribution
4. Manual review and merge

**Pros:**
- ✅ Best of both worlds
- ✅ Self-improving without per-run overhead
- ✅ Transparent updates via PR
- ✅ Gradual improvement

**Cons:**
- ❌ Most complex to set up
- ❌ Needs scheduled workflow + automation

**Effort:** 6-8 hours

---

## Recommendation

### Short-term (Immediate ~90% improvement)
**Use Approach 1: Static Weight-Based Chunking**

Rationale:
- Complexity analysis shows clear imbalance (Chunk 1 is 2.8x heavier)
- 90% imbalance reduction achievable in 1-2 hours
- No runtime overhead or external dependencies
- Can measure actual improvement in next CI run

### Long-term (Optimal performance)
**Migrate to Approach 4: Hybrid with Periodic Updates**

Rationale:
- Adapts to test suite evolution
- Uses real timing data
- Minimal per-run overhead
- Automated maintenance

---

## Expected Performance Improvement

### Current State (File-count based)
- Slowest chunk (Chunk 1): ~5220 weight units
- Fastest chunk (Chunk 3): ~1823 weight units
- Ratio: **2.86x difference**
- Parallel speedup limited by slowest chunk

### After Optimization (Weight-based)
- Slowest chunk: ~3771 weight units
- Fastest chunk: ~3441 weight units
- Ratio: **1.10x difference**
- Better parallelization efficiency

### Wall-clock Time Estimate
If current chunks take:
- Chunk 0: 3 min
- Chunk 1: 5.3 min ⚠️ (bottleneck)
- Chunk 2: 4.5 min
- Chunk 3: 1.8 min

**Total CI time: 5.3 minutes** (limited by Chunk 1)

After optimization (assuming even ~3.6 min per chunk):
**Total CI time: 3.6 minutes**

**Estimated improvement: ~32% faster CI** (5.3 → 3.6 min)

---

## Next Steps (If Implementing)

1. **Verify hypothesis:**
   - Wait for current PR CI to complete
   - Check actual chunk timings in GitHub Actions
   - Confirm imbalance matches predictions

2. **Implement static optimization:**
   - Update `.github/scripts/chunk-tests.sh` with optimal distribution
   - Test locally with all 4 chunks
   - Measure improvement in CI

3. **Monitor and iterate:**
   - Track chunk timings over multiple runs
   - Adjust if test execution patterns change
   - Consider automated approach when ROI justifies it
